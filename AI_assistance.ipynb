{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzyeiXy5SybS",
        "outputId": "f9ca8c02-92b0-4824-f1f3-82056d44c011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.49)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.19)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.11.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.4.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 typing-inspect-0.9.0\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.49)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.69.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.3.19)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (4.13.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-openai\n",
            "Successfully installed langchain-openai-0.3.12 tiktoken-0.9.0\n",
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.11/dist-packages (2025.4.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.1.8)\n",
            "Requirement already satisfied: primp>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (0.14.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.3.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.69.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain\n",
        "! pip install langchain-community\n",
        "! pip install langchain-openai\n",
        "! pip install duckduckgo-search\n",
        "! pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VXgYEWsiMJW",
        "outputId": "7248e175-104b-42fb-d38e-ff8db05345d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================ CONFIGURACI√ìN ================================\n",
            "‚ùì Pregunta: ¬øComo programar y entrenar un mamba llm?\n",
            "üî¢ N√∫mero de resultados: 4\n",
            "\n",
            "============================== INICIO DE PROCESO ==============================\n",
            "üîç Procesando pregunta: ¬øComo programar y entrenar un mamba llm?\n",
            "üîç N√∫mero de resultados a procesar: 4\n",
            "\n",
            "=================================== B√öSQUEDA ===================================\n",
            "üîç Realizando b√∫squeda en DuckDuckGo: '¬øComo programar y entrenar un mamba llm?'\n",
            "üîç Solicitando 4 resultados\n",
            "‚úÖ B√∫squeda completada en 0.82 segundos\n",
            "‚úÖ Se encontraron 4 resultados:\n",
            "  [1] Introducci√≥n a la arquitectura Mamba LLM: Un nuevo ... - DataCamp\n",
            "      URL: https://www.datacamp.com/es/tutorial/introduction-to-the-mamba-llm-architecture\n",
            "  [2] Entrenamiento de LLM con Datasets Gu√≠a Paso a Paso\n",
            "      URL: https://decolornaranja.wordpress.com/2024/09/05/como-entrenar-un-modelo-de-lenguaje-llm-usando-un-dataset-paso-a-paso/\n",
            "  [3] Una gu√≠a paso a paso para entrenar modelos de lenguaje grandes (LLM ...\n",
            "      URL: https://blogs.novita.ai/es/a-step-by-step-guide-to-training-large-language-models-llms-on-your-own-data/\n",
            "  [4] Ejecutar modelos LLM en local sin GPU - Agentes de IA\n",
            "      URL: https://www.agentesdeia.com/ejecutar-modelos-llm-local/\n",
            "\n",
            "============================ AN√ÅLISIS DE RESULTADOS ============================\n",
            "üîç Procesando 4 resultados de b√∫squeda\n",
            "\n",
            "============================= PROCESANDO P√ÅGINA 1 =============================\n",
            "üìÑ Cargando p√°gina 1: Introducci√≥n a la arquitectura Mamba LLM: Un nuevo ... - DataCamp\n",
            "üìÑ URL: https://www.datacamp.com/es/tutorial/introduction-to-the-mamba-llm-architecture\n",
            "üì• Descargando contenido...\n",
            "üì• Descarga completada en 0.12 segundos\n",
            "üìä ESTAD√çSTICAS DEL DOCUMENTO:\n",
            "  ‚Ä¢ Caracteres: 57\n",
            "  ‚Ä¢ Palabras aproximadas: 8\n",
            "  ‚Ä¢ Tokens estimados: ~14 (estimaci√≥n basada en 4 caracteres/token)\n",
            "‚úÇÔ∏è Dividiendo documento en chunks...\n",
            "‚úÇÔ∏è Divisi√≥n completada en 0.00 segundos\n",
            "‚úÇÔ∏è Documento dividido en 1 chunks\n",
            "  ‚Ä¢ Chunk 1: 57 caracteres, ~8 palabras, ~14 tokens\n",
            "‚ÑπÔ∏è La p√°gina 1 tiene un solo chunk, no requiere resumen\n",
            "\n",
            "============================= PROCESANDO P√ÅGINA 2 =============================\n",
            "üìÑ Cargando p√°gina 2: Entrenamiento de LLM con Datasets Gu√≠a Paso a Paso\n",
            "üìÑ URL: https://decolornaranja.wordpress.com/2024/09/05/como-entrenar-un-modelo-de-lenguaje-llm-usando-un-dataset-paso-a-paso/\n",
            "üì• Descargando contenido...\n",
            "üì• Descarga completada en 0.28 segundos\n",
            "üìä ESTAD√çSTICAS DEL DOCUMENTO:\n",
            "  ‚Ä¢ Caracteres: 14,924\n",
            "  ‚Ä¢ Palabras aproximadas: 2,214\n",
            "  ‚Ä¢ Tokens estimados: ~3,731 (estimaci√≥n basada en 4 caracteres/token)\n",
            "‚úÇÔ∏è Dividiendo documento en chunks...\n",
            "‚úÇÔ∏è Divisi√≥n completada en 0.00 segundos\n",
            "‚úÇÔ∏è Documento dividido en 6 chunks\n",
            "  ‚Ä¢ Chunk 1: 1,038 caracteres, ~153 palabras, ~259 tokens\n",
            "  ‚Ä¢ Chunk 2: 3,718 caracteres, ~582 palabras, ~929 tokens\n",
            "  ‚Ä¢ Chunk 3: 3,183 caracteres, ~484 palabras, ~795 tokens\n",
            "  ‚Ä¢ Chunk 4: 3,887 caracteres, ~614 palabras, ~971 tokens\n",
            "  ‚Ä¢ Chunk 5: 725 caracteres, ~107 palabras, ~181 tokens\n",
            "  ‚Ä¢ Chunk 6: 2,413 caracteres, ~284 palabras, ~603 tokens\n",
            "\n",
            "============================= RESUMIENDO P√ÅGINA 2 =============================\n",
            "üìù Resumiendo p√°gina 2: Entrenamiento de LLM con Datasets Gu√≠a Paso a Paso\n",
            "üìù Procesando 6 chunks\n",
            "ü§ñ Inicializando modelo GPT-3.5-turbo para resumir\n",
            "üìã Configurando prompts de mapeo y reducci√≥n\n",
            "‚öôÔ∏è Creando cadena de resumen Map-Reduce\n",
            "üîÑ Ejecutando cadena de resumen Map-Reduce...\n",
            "üîÑ Fase de Map: Procesando 6 chunks individualmente\n",
            "‚úÖ Resumen completado en 18.45 segundos\n",
            "üìä ESTAD√çSTICAS DEL RESUMEN:\n",
            "  ‚Ä¢ Caracteres: 1,349\n",
            "  ‚Ä¢ Palabras aproximadas: 206\n",
            "  ‚Ä¢ Tokens estimados: ~337\n",
            "  ‚Ä¢ Ratio de compresi√≥n: 9.0% del original\n",
            "\n",
            "============================= PROCESANDO P√ÅGINA 3 =============================\n",
            "üìÑ Cargando p√°gina 3: Una gu√≠a paso a paso para entrenar modelos de lenguaje grandes (LLM ...\n",
            "üìÑ URL: https://blogs.novita.ai/es/a-step-by-step-guide-to-training-large-language-models-llms-on-your-own-data/\n",
            "üì• Descargando contenido...\n",
            "üì• Descarga completada en 1.31 segundos\n",
            "üìä ESTAD√çSTICAS DEL DOCUMENTO:\n",
            "  ‚Ä¢ Caracteres: 22,303\n",
            "  ‚Ä¢ Palabras aproximadas: 3,309\n",
            "  ‚Ä¢ Tokens estimados: ~5,575 (estimaci√≥n basada en 4 caracteres/token)\n",
            "‚úÇÔ∏è Dividiendo documento en chunks...\n",
            "‚úÇÔ∏è Divisi√≥n completada en 0.00 segundos\n",
            "‚úÇÔ∏è Documento dividido en 8 chunks\n",
            "  ‚Ä¢ Chunk 1: 3,789 caracteres, ~530 palabras, ~947 tokens\n",
            "  ‚Ä¢ Chunk 2: 2,273 caracteres, ~367 palabras, ~568 tokens\n",
            "  ‚Ä¢ Chunk 3: 3,817 caracteres, ~580 palabras, ~954 tokens\n",
            "  ‚Ä¢ Chunk 4: 249 caracteres, ~35 palabras, ~62 tokens\n",
            "  ‚Ä¢ Chunk 5: 3,818 caracteres, ~589 palabras, ~954 tokens\n",
            "  ‚Ä¢ Chunk 6: 3,400 caracteres, ~494 palabras, ~850 tokens\n",
            "  ‚Ä¢ Chunk 7: 3,857 caracteres, ~564 palabras, ~964 tokens\n",
            "  ‚Ä¢ Chunk 8: 1,242 caracteres, ~173 palabras, ~310 tokens\n",
            "\n",
            "============================= RESUMIENDO P√ÅGINA 3 =============================\n",
            "üìù Resumiendo p√°gina 3: Una gu√≠a paso a paso para entrenar modelos de lenguaje grandes (LLM ...\n",
            "üìù Procesando 8 chunks\n",
            "ü§ñ Inicializando modelo GPT-3.5-turbo para resumir\n",
            "üìã Configurando prompts de mapeo y reducci√≥n\n",
            "‚öôÔ∏è Creando cadena de resumen Map-Reduce\n",
            "üîÑ Ejecutando cadena de resumen Map-Reduce...\n",
            "üîÑ Fase de Map: Procesando 8 chunks individualmente\n",
            "‚úÖ Resumen completado en 32.28 segundos\n",
            "üìä ESTAD√çSTICAS DEL RESUMEN:\n",
            "  ‚Ä¢ Caracteres: 2,292\n",
            "  ‚Ä¢ Palabras aproximadas: 330\n",
            "  ‚Ä¢ Tokens estimados: ~573\n",
            "  ‚Ä¢ Ratio de compresi√≥n: 10.3% del original\n",
            "\n",
            "============================= PROCESANDO P√ÅGINA 4 =============================\n",
            "üìÑ Cargando p√°gina 4: Ejecutar modelos LLM en local sin GPU - Agentes de IA\n",
            "üìÑ URL: https://www.agentesdeia.com/ejecutar-modelos-llm-local/\n",
            "üì• Descargando contenido...\n",
            "üì• Descarga completada en 1.01 segundos\n",
            "üìä ESTAD√çSTICAS DEL DOCUMENTO:\n",
            "  ‚Ä¢ Caracteres: 10,484\n",
            "  ‚Ä¢ Palabras aproximadas: 1,641\n",
            "  ‚Ä¢ Tokens estimados: ~2,621 (estimaci√≥n basada en 4 caracteres/token)\n",
            "‚úÇÔ∏è Dividiendo documento en chunks...\n",
            "‚úÇÔ∏è Divisi√≥n completada en 0.00 segundos\n",
            "‚úÇÔ∏è Documento dividido en 4 chunks\n",
            "  ‚Ä¢ Chunk 1: 2,242 caracteres, ~351 palabras, ~560 tokens\n",
            "  ‚Ä¢ Chunk 2: 2,444 caracteres, ~401 palabras, ~611 tokens\n",
            "  ‚Ä¢ Chunk 3: 3,673 caracteres, ~594 palabras, ~918 tokens\n",
            "  ‚Ä¢ Chunk 4: 2,382 caracteres, ~350 palabras, ~595 tokens\n",
            "\n",
            "============================= RESUMIENDO P√ÅGINA 4 =============================\n",
            "üìù Resumiendo p√°gina 4: Ejecutar modelos LLM en local sin GPU - Agentes de IA\n",
            "üìù Procesando 4 chunks\n",
            "ü§ñ Inicializando modelo GPT-3.5-turbo para resumir\n",
            "üìã Configurando prompts de mapeo y reducci√≥n\n",
            "‚öôÔ∏è Creando cadena de resumen Map-Reduce\n",
            "üîÑ Ejecutando cadena de resumen Map-Reduce...\n",
            "üîÑ Fase de Map: Procesando 4 chunks individualmente\n",
            "‚úÖ Resumen completado en 13.56 segundos\n",
            "üìä ESTAD√çSTICAS DEL RESUMEN:\n",
            "  ‚Ä¢ Caracteres: 1,075\n",
            "  ‚Ä¢ Palabras aproximadas: 162\n",
            "  ‚Ä¢ Tokens estimados: ~268\n",
            "  ‚Ä¢ Ratio de compresi√≥n: 10.3% del original\n",
            "\n",
            "======================== PREPARACI√ìN DEL CONTEXTO FINAL ========================\n",
            "üìã Preparando contexto final para respuesta...\n",
            "  ‚Ä¢ Fuente 1: Introducci√≥n a la arquitectura Mamba LLM: Un nuevo ... - DataCamp (57 caracteres)\n",
            "  ‚Ä¢ Fuente 2: Entrenamiento de LLM con Datasets Gu√≠a Paso a Paso (1,349 caracteres)\n",
            "  ‚Ä¢ Fuente 3: Una gu√≠a paso a paso para entrenar modelos de lenguaje grandes (LLM ... (2,292 caracteres)\n",
            "  ‚Ä¢ Fuente 4: Ejecutar modelos LLM en local sin GPU - Agentes de IA (1,075 caracteres)\n",
            "üìä ESTAD√çSTICAS DEL CONTEXTO FINAL:\n",
            "  ‚Ä¢ Total caracteres: 5,469\n",
            "  ‚Ä¢ Tokens estimados: ~1,367\n",
            "\n",
            "======================== GENERACI√ìN DE RESPUESTA FINAL ========================\n",
            "ü§ñ Inicializando modelo para respuesta final...\n",
            "ü§ñ Usando modelo gpt-3.5-turbo est√°ndar\n",
            "üîÑ Enviando contexto a GPT para respuesta final...\n",
            "‚úÖ Respuesta generada en 10.56 segundos\n",
            "üìä ESTAD√çSTICAS DE LA RESPUESTA FINAL:\n",
            "  ‚Ä¢ Caracteres: 2,526\n",
            "  ‚Ä¢ Palabras aproximadas: 372\n",
            "\n",
            "============================== PROCESO COMPLETADO ==============================\n",
            "‚è±Ô∏è Tiempo total de ejecuci√≥n: 78.90 segundos\n",
            "\n",
            "=============================== RESULTADO FINAL ===============================\n",
            "Para programar y entrenar un modelo de lenguaje grande (LLM) como un Mamba LLM, es fundamental seguir una serie de pasos y consideraciones clave. \n",
            "\n",
            "1. **Selecci√≥n del dataset adecuado:** Se debe elegir un dataset diverso y de alta calidad, posiblemente etiquetado dependiendo de la tarea espec√≠fica [FUENTE 2].\n",
            "\n",
            "2. **Preprocesamiento de datos:** Es importante realizar limpieza, tokenizaci√≥n, normalizaci√≥n y filtrado de los datos antes de entrenar el modelo [FUENTE 2].\n",
            "\n",
            "3. **Configuraci√≥n del modelo:** Se debe elegir una arquitectura adecuada como GPT, BERT o T5 y ajustar los hiperpar√°metros durante el entrenamiento utilizando algoritmos de optimizaci√≥n como Adam o SGD [FUENTE 2].\n",
            "\n",
            "4. **Entrenamiento y evaluaci√≥n:** Durante el entrenamiento, se debe monitorear el modelo para evitar el sobreajuste y evaluarlo en un conjunto de datos de prueba. Se pueden ajustar hiperpar√°metros o la arquitectura si es necesario [FUENTE 2].\n",
            "\n",
            "5. **Selecci√≥n de marco de aprendizaje profundo y arquitectura:** Es crucial elegir un marco como TensorFlow, PyTorch o Hugging Face Transformers, y utilizar la arquitectura Transformer como punto de partida com√∫n para un Mamba LLM [FUENTE 3].\n",
            "\n",
            "6. **Recursos y tiempo de entrenamiento:** Se debe tener en cuenta que el entrenamiento de un modelo LLM puede consumir recursos computacionales y tiempo considerable, con duraciones que van desde horas hasta semanas [FUENTE 3].\n",
            "\n",
            "7. **Validaci√≥n y ajuste fino:** Es importante validar y evaluar peri√≥dicamente el rendimiento del modelo, ajustarlo finamente si es necesario, y probarlo con datos del mundo real antes de implementarlo para los usuarios [FUENTE 3].\n",
            "\n",
            "8. **Herramientas y modelos disponibles:** Se pueden utilizar herramientas como GPT4All, LMStudio, AnythingLLM y Ollama para programar y entrenar un modelo LLM, con requisitos de hardware espec√≠ficos. Adem√°s, existen modelos como Gemma, Llama, Phi3, Openchat, entre otros, que se pueden utilizar en experimentos [FUENTE 4].\n",
            "\n",
            "En resumen, para programar y entrenar un Mamba LLM, se deben seguir los pasos mencionados anteriormente, considerando la selecci√≥n del dataset, el preprocesamiento de los datos, la configuraci√≥n del modelo, el entrenamiento, la evaluaci√≥n, la selecci√≥n de marco de aprendizaje y arquitectura, la gesti√≥n de recursos y tiempo, la validaci√≥n y ajuste fino, y la utilizaci√≥n de herramientas y modelos disponibles. Se recomienda consultar fuentes adicionales para obtener informaci√≥n m√°s detallada sobre la programaci√≥n y entrenamiento espec√≠fico de un Mamba LLM.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Configurar la API key de OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "def log_separator(message=\"\"):\n",
        "    \"\"\"Imprime un separador con un mensaje opcional\"\"\"\n",
        "    width = 80\n",
        "    if message:\n",
        "        padding = (width - len(message) - 2) // 2\n",
        "        print(\"\\n\" + \"=\" * padding + f\" {message} \" + \"=\" * padding)\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\" * width)\n",
        "\n",
        "def realizar_busqueda(query: str, num_results: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Realiza una b√∫squeda en DuckDuckGo y devuelve los resultados\n",
        "    \"\"\"\n",
        "    log_separator(\"B√öSQUEDA\")\n",
        "    print(f\"üîç Realizando b√∫squeda en DuckDuckGo: '{query}'\")\n",
        "    print(f\"üîç Solicitando {num_results} resultados\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    search = DuckDuckGoSearchAPIWrapper()\n",
        "    results = search.results(query, num_results)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"‚úÖ B√∫squeda completada en {elapsed:.2f} segundos\")\n",
        "    print(f\"‚úÖ Se encontraron {len(results)} resultados:\")\n",
        "\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"  [{i}] {result.get('title', 'Sin t√≠tulo')}\")\n",
        "        print(f\"      URL: {result.get('link', 'Sin enlace')}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def cargar_y_procesar_pagina(url: str, title: str, index: int) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Carga y procesa una p√°gina web completa, dividi√©ndola en chunks si es necesario\n",
        "    \"\"\"\n",
        "    log_separator(f\"PROCESANDO P√ÅGINA {index}\")\n",
        "    print(f\"üìÑ Cargando p√°gina {index}: {title}\")\n",
        "    print(f\"üìÑ URL: {url}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        print(f\"üì• Descargando contenido...\")\n",
        "        loader = WebBaseLoader(url)\n",
        "        documents = loader.load()\n",
        "        download_time = time.time() - start_time\n",
        "        print(f\"üì• Descarga completada en {download_time:.2f} segundos\")\n",
        "\n",
        "        if not documents:\n",
        "            print(f\"‚ö†Ô∏è AVISO: No se pudo extraer contenido de {url}\")\n",
        "            return {\n",
        "                \"title\": title,\n",
        "                \"url\": url,\n",
        "                \"success\": False,\n",
        "                \"content\": None,\n",
        "                \"chunks\": [],\n",
        "                \"error\": \"No se pudo extraer contenido\",\n",
        "                \"stats\": {\n",
        "                    \"download_time\": download_time,\n",
        "                    \"processing_time\": 0,\n",
        "                    \"total_time\": download_time\n",
        "                }\n",
        "            }\n",
        "\n",
        "        content = documents[0].page_content\n",
        "        content_chars = len(content)\n",
        "        content_words = len(content.split())\n",
        "\n",
        "        print(f\"üìä ESTAD√çSTICAS DEL DOCUMENTO:\")\n",
        "        print(f\"  ‚Ä¢ Caracteres: {content_chars:,}\")\n",
        "        print(f\"  ‚Ä¢ Palabras aproximadas: {content_words:,}\")\n",
        "        print(f\"  ‚Ä¢ Tokens estimados: ~{int(content_chars/4):,} (estimaci√≥n basada en 4 caracteres/token)\")\n",
        "\n",
        "        # Dividir el contenido en chunks manejables\n",
        "        print(f\"‚úÇÔ∏è Dividiendo documento en chunks...\")\n",
        "        chunk_start = time.time()\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=4000,         # Tama√±o aproximado de cada chunk\n",
        "            chunk_overlap=200,       # Solapamiento entre chunks para mantener contexto\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Prioridad de separadores\n",
        "        )\n",
        "\n",
        "        chunks = text_splitter.split_text(content)\n",
        "        chunk_time = time.time() - chunk_start\n",
        "\n",
        "        print(f\"‚úÇÔ∏è Divisi√≥n completada en {chunk_time:.2f} segundos\")\n",
        "        print(f\"‚úÇÔ∏è Documento dividido en {len(chunks)} chunks\")\n",
        "\n",
        "        # Mostrar estad√≠sticas de los chunks\n",
        "        for i, chunk in enumerate(chunks, 1):\n",
        "            chunk_chars = len(chunk)\n",
        "            chunk_words = len(chunk.split())\n",
        "            print(f\"  ‚Ä¢ Chunk {i}: {chunk_chars:,} caracteres, ~{chunk_words:,} palabras, ~{int(chunk_chars/4):,} tokens\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"url\": url,\n",
        "            \"success\": True,\n",
        "            \"content\": content,  # Contenido completo\n",
        "            \"chunks\": chunks,    # Contenido dividido en chunks\n",
        "            \"error\": None,\n",
        "            \"stats\": {\n",
        "                \"download_time\": download_time,\n",
        "                \"processing_time\": chunk_time,\n",
        "                \"total_time\": total_time,\n",
        "                \"total_chars\": content_chars,\n",
        "                \"total_words\": content_words,\n",
        "                \"num_chunks\": len(chunks),\n",
        "                \"chunk_sizes\": [len(chunk) for chunk in chunks]\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"‚ùå ERROR: Error al procesar {url}: {str(e)}\")\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"url\": url,\n",
        "            \"success\": False,\n",
        "            \"content\": None,\n",
        "            \"chunks\": [],\n",
        "            \"error\": str(e),\n",
        "            \"stats\": {\n",
        "                \"download_time\": elapsed,\n",
        "                \"processing_time\": 0,\n",
        "                \"total_time\": elapsed\n",
        "            }\n",
        "        }\n",
        "\n",
        "def resumir_pagina_larga(pagina: Dict[str, Any], pregunta: str, index: int) -> str:\n",
        "    \"\"\"\n",
        "    Procesa una p√°gina muy larga dividida en chunks y produce un resumen\n",
        "    \"\"\"\n",
        "    if not pagina[\"success\"] or not pagina[\"chunks\"]:\n",
        "        return f\"No se pudo analizar la p√°gina: {pagina['error'] or 'Contenido no disponible'}\"\n",
        "\n",
        "    log_separator(f\"RESUMIENDO P√ÅGINA {index}\")\n",
        "    print(f\"üìù Resumiendo p√°gina {index}: {pagina['title']}\")\n",
        "    print(f\"üìù Procesando {len(pagina['chunks'])} chunks\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Convertir los chunks a formato Document de LangChain\n",
        "    chunks_docs = [\n",
        "        Document(page_content=chunk, metadata={\"source\": pagina[\"url\"], \"title\": pagina[\"title\"]})\n",
        "        for chunk in pagina[\"chunks\"]\n",
        "    ]\n",
        "\n",
        "    # Inicializar el modelo\n",
        "    print(f\"ü§ñ Inicializando modelo GPT-3.5-turbo para resumir\")\n",
        "    llm = ChatOpenAI(temperature=0.3, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Prompt para resumir cada chunk\n",
        "    print(f\"üìã Configurando prompts de mapeo y reducci√≥n\")\n",
        "    map_prompt_template = \"\"\"\n",
        "    Tu tarea es extraer informaci√≥n relevante del siguiente fragmento de una p√°gina web\n",
        "    que responda a esta pregunta del usuario: \"{question}\"\n",
        "\n",
        "    Solo extrae informaci√≥n directamente relacionada con la pregunta. S√© conciso.\n",
        "\n",
        "    FRAGMENTO DE P√ÅGINA WEB:\n",
        "    {text}\n",
        "\n",
        "    INFORMACI√ìN RELEVANTE EXTRA√çDA (solo si est√° relacionada con la pregunta):\n",
        "    \"\"\"\n",
        "\n",
        "    map_prompt = PromptTemplate(\n",
        "        template=map_prompt_template,\n",
        "        input_variables=[\"text\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Prompt para combinar los res√∫menes de los chunks\n",
        "    combine_prompt_template = \"\"\"\n",
        "    Has recibido extractos de informaci√≥n de diferentes partes de una p√°gina web titulada \"{title}\".\n",
        "    Usa estos extractos para crear un resumen coherente que responda a la pregunta del usuario.\n",
        "\n",
        "    PREGUNTA DEL USUARIO: {question}\n",
        "\n",
        "    EXTRACTOS DE LA P√ÅGINA WEB:\n",
        "    {text}\n",
        "\n",
        "    RESUMEN COHERENTE Y COMPLETO:\n",
        "    \"\"\"\n",
        "\n",
        "    combine_prompt = PromptTemplate(\n",
        "        template=combine_prompt_template,\n",
        "        input_variables=[\"title\", \"text\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Cargar la cadena de resumen\n",
        "    print(f\"‚öôÔ∏è Creando cadena de resumen Map-Reduce\")\n",
        "    summary_chain = load_summarize_chain(\n",
        "        llm,\n",
        "        chain_type=\"map_reduce\",\n",
        "        map_prompt=map_prompt,\n",
        "        combine_prompt=combine_prompt,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Ejecutar la cadena de resumen con los chunks\n",
        "        print(f\"üîÑ Ejecutando cadena de resumen Map-Reduce...\")\n",
        "        print(f\"üîÑ Fase de Map: Procesando {len(chunks_docs)} chunks individualmente\")\n",
        "        summary_start = time.time()\n",
        "\n",
        "        summary = summary_chain.invoke({\n",
        "            \"input_documents\": chunks_docs,\n",
        "            \"question\": pregunta,\n",
        "            \"title\": pagina[\"title\"]\n",
        "        })\n",
        "\n",
        "        summary_time = time.time() - summary_start\n",
        "        print(f\"‚úÖ Resumen completado en {summary_time:.2f} segundos\")\n",
        "\n",
        "        # Extraer el resultado\n",
        "        if isinstance(summary, dict) and \"output_text\" in summary:\n",
        "            result = summary[\"output_text\"]\n",
        "        else:\n",
        "            result = str(summary)\n",
        "\n",
        "        result_chars = len(result)\n",
        "        result_words = len(result.split())\n",
        "\n",
        "        print(f\"üìä ESTAD√çSTICAS DEL RESUMEN:\")\n",
        "        print(f\"  ‚Ä¢ Caracteres: {result_chars:,}\")\n",
        "        print(f\"  ‚Ä¢ Palabras aproximadas: {result_words:,}\")\n",
        "        print(f\"  ‚Ä¢ Tokens estimados: ~{int(result_chars/4):,}\")\n",
        "        print(f\"  ‚Ä¢ Ratio de compresi√≥n: {result_chars/pagina['stats']['total_chars']*100:.1f}% del original\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"‚ùå ERROR: Error al resumir: {str(e)}\")\n",
        "        return f\"Error al resumir la p√°gina: {str(e)}\"\n",
        "\n",
        "def analizar_resultados_completos(resultados_busqueda: List[Dict[str, Any]], pregunta: str) -> str:\n",
        "    \"\"\"\n",
        "    Procesa los resultados de b√∫squeda, carga cada p√°gina web, y genera una respuesta\n",
        "    \"\"\"\n",
        "    log_separator(\"AN√ÅLISIS DE RESULTADOS\")\n",
        "    print(f\"üîç Procesando {len(resultados_busqueda)} resultados de b√∫squeda\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Procesar cada p√°gina encontrada\n",
        "    paginas_procesadas = []\n",
        "\n",
        "    for i, result in enumerate(resultados_busqueda, 1):\n",
        "        title = result.get(\"title\", \"Sin t√≠tulo\")\n",
        "        url = result.get(\"link\", \"Sin enlace\")\n",
        "\n",
        "        # Cargar y procesar la p√°gina\n",
        "        pagina_procesada = cargar_y_procesar_pagina(url, title, i)\n",
        "\n",
        "        if pagina_procesada[\"success\"]:\n",
        "            # Si la p√°gina es muy larga (tiene m√∫ltiples chunks), resumirla\n",
        "            if len(pagina_procesada[\"chunks\"]) > 1:\n",
        "                resumen = resumir_pagina_larga(pagina_procesada, pregunta, i)\n",
        "                pagina_procesada[\"resumen\"] = resumen\n",
        "            # Si la p√°gina es corta, usar el contenido directamente\n",
        "            else:\n",
        "                print(f\"‚ÑπÔ∏è La p√°gina {i} tiene un solo chunk, no requiere resumen\")\n",
        "                pagina_procesada[\"resumen\"] = pagina_procesada[\"content\"]\n",
        "\n",
        "        paginas_procesadas.append(pagina_procesada)\n",
        "\n",
        "    # Preparar el contexto final para GPT\n",
        "    log_separator(\"PREPARACI√ìN DEL CONTEXTO FINAL\")\n",
        "    print(f\"üìã Preparando contexto final para respuesta...\")\n",
        "\n",
        "    contexto_items = []\n",
        "    total_contexto_chars = 0\n",
        "\n",
        "    for i, pagina in enumerate(paginas_procesadas, 1):\n",
        "        if pagina[\"success\"]:\n",
        "            contenido = pagina.get('resumen', pagina['content'])\n",
        "            contexto_item = f\"[FUENTE {i}] {pagina['title']}\\n[URL] {pagina['url']}\\n\\n{contenido}\"\n",
        "            chars = len(contenido)\n",
        "            total_contexto_chars += chars\n",
        "            print(f\"  ‚Ä¢ Fuente {i}: {pagina['title']} ({chars:,} caracteres)\")\n",
        "        else:\n",
        "            contexto_item = f\"[FUENTE {i}] {pagina['title']}\\n[URL] {pagina['url']}\\n\\nNo se pudo obtener el contenido: {pagina['error']}\"\n",
        "            print(f\"  ‚Ä¢ Fuente {i}: {pagina['title']} (NO DISPONIBLE - {pagina['error']})\")\n",
        "\n",
        "        contexto_items.append(contexto_item)\n",
        "\n",
        "    # Unir el contexto final\n",
        "    contexto_final = \"\\n\\n---\\n\\n\".join(contexto_items)\n",
        "    contexto_final_chars = len(contexto_final)\n",
        "    contexto_final_tokens = int(contexto_final_chars/4)\n",
        "\n",
        "    print(f\"üìä ESTAD√çSTICAS DEL CONTEXTO FINAL:\")\n",
        "    print(f\"  ‚Ä¢ Total caracteres: {contexto_final_chars:,}\")\n",
        "    print(f\"  ‚Ä¢ Tokens estimados: ~{contexto_final_tokens:,}\")\n",
        "\n",
        "    if contexto_final_tokens > 15000:\n",
        "        print(f\"‚ö†Ô∏è ADVERTENCIA: El contexto final excede los 15,000 tokens aproximados.\")\n",
        "        print(f\"‚ö†Ô∏è Se usar√° gpt-3.5-turbo-16k que puede manejar hasta ~16k tokens.\")\n",
        "\n",
        "    # Generar la respuesta final con GPT\n",
        "    log_separator(\"GENERACI√ìN DE RESPUESTA FINAL\")\n",
        "    print(f\"ü§ñ Inicializando modelo para respuesta final...\")\n",
        "\n",
        "    template = \"\"\"\n",
        "    Responde a la siguiente pregunta bas√°ndote en el contenido de las p√°ginas web proporcionadas.\n",
        "    Para cada afirmaci√≥n importante en tu respuesta, cita la fuente correspondiente usando [FUENTE X].\n",
        "\n",
        "    PREGUNTA: {question}\n",
        "\n",
        "    CONTENIDO DE LAS P√ÅGINAS WEB:\n",
        "    {context}\n",
        "\n",
        "    RESPUESTA DETALLADA (citando las fuentes):\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=template\n",
        "    )\n",
        "\n",
        "    # Usar un modelo con contexto amplio para la respuesta final\n",
        "    if contexto_final_tokens > 15000:\n",
        "        print(f\"ü§ñ Usando modelo gpt-3.5-turbo-16k para manejar el contexto amplio\")\n",
        "        llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo-16k\")\n",
        "    else:\n",
        "        print(f\"ü§ñ Usando modelo gpt-3.5-turbo est√°ndar\")\n",
        "        llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    try:\n",
        "        print(f\"üîÑ Enviando contexto a GPT para respuesta final...\")\n",
        "        resp_start = time.time()\n",
        "\n",
        "        respuesta = chain.invoke({\n",
        "            \"context\": contexto_final,\n",
        "            \"question\": pregunta\n",
        "        })\n",
        "\n",
        "        resp_time = time.time() - resp_start\n",
        "        print(f\"‚úÖ Respuesta generada en {resp_time:.2f} segundos\")\n",
        "\n",
        "        if isinstance(respuesta, dict) and \"text\" in respuesta:\n",
        "            result = respuesta[\"text\"]\n",
        "        else:\n",
        "            result = respuesta\n",
        "\n",
        "        result_chars = len(result)\n",
        "        result_words = len(result.split())\n",
        "\n",
        "        print(f\"üìä ESTAD√çSTICAS DE LA RESPUESTA FINAL:\")\n",
        "        print(f\"  ‚Ä¢ Caracteres: {result_chars:,}\")\n",
        "        print(f\"  ‚Ä¢ Palabras aproximadas: {result_words:,}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR: Error al generar respuesta final: {str(e)}\")\n",
        "        return f\"Error al generar la respuesta: {str(e)}\"\n",
        "\n",
        "def buscar_y_responder(pregunta: str, num_resultados: int = 4) -> str:\n",
        "    \"\"\"\n",
        "    Funci√≥n principal que gestiona todo el proceso\n",
        "    \"\"\"\n",
        "    log_separator(\"INICIO DE PROCESO\")\n",
        "    print(f\"üîç Procesando pregunta: {pregunta}\")\n",
        "    print(f\"üîç N√∫mero de resultados a procesar: {num_resultados}\")\n",
        "\n",
        "    global_start = time.time()\n",
        "\n",
        "    # Realizar b√∫squeda\n",
        "    resultados = realizar_busqueda(pregunta, num_resultados)\n",
        "\n",
        "    # Analizar resultados y generar respuesta\n",
        "    respuesta = analizar_resultados_completos(resultados, pregunta)\n",
        "\n",
        "    total_time = time.time() - global_start\n",
        "\n",
        "    log_separator(\"PROCESO COMPLETADO\")\n",
        "    print(f\"‚è±Ô∏è Tiempo total de ejecuci√≥n: {total_time:.2f} segundos\")\n",
        "\n",
        "    return respuesta\n",
        "\n",
        "# Ejemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    pregunta_usuario = \"¬øComo programar y entrenar un mamba llm?\"\n",
        "    num_resultados = 4\n",
        "\n",
        "    log_separator(\"CONFIGURACI√ìN\")\n",
        "    print(f\"‚ùì Pregunta: {pregunta_usuario}\")\n",
        "    print(f\"üî¢ N√∫mero de resultados: {num_resultados}\")\n",
        "\n",
        "    respuesta = buscar_y_responder(pregunta_usuario, num_resultados)\n",
        "\n",
        "    log_separator(\"RESULTADO FINAL\")\n",
        "    print(respuesta)\n",
        "    log_separator()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
