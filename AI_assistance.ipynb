{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BzyeiXy5SybS",
        "outputId": "f9ca8c02-92b0-4824-f1f3-82056d44c011"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.21)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.49)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.7)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.19)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.0)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.17->langchain) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain) (3.0.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.17->langchain) (1.3.1)\n",
            "Collecting langchain-community\n",
            "  Downloading langchain_community-0.3.20-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.49)\n",
            "Requirement already satisfied: langchain<1.0.0,>=0.3.21 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.21)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.40)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (9.0.0)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n",
            "  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.19)\n",
            "Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: numpy<3,>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.18.3)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.7 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (0.3.7)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.21->langchain-community) (2.11.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (1.33)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-community) (4.13.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.28.1)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-community) (0.23.0)\n",
            "Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.1.31)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.1.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (4.9.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-community) (3.0.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.21->langchain-community) (0.4.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.4,>=0.1.125->langchain-community) (1.3.1)\n",
            "Downloading langchain_community-0.3.20-py3-none-any.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Installing collected packages: mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-community\n",
            "Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.0 langchain-community-0.3.20 marshmallow-3.26.1 mypy-extensions-1.0.0 pydantic-settings-2.8.1 typing-inspect-0.9.0\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.3.12-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.49 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.49)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.68.2 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.69.0)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.3.19)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (9.0.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (6.0.2)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (4.13.0)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.11.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (0.9.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.68.2->langchain-openai) (4.67.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.68.2->langchain-openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.68.2->langchain-openai) (0.14.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.49->langchain-openai) (3.0.0)\n",
            "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (3.10.16)\n",
            "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (1.0.0)\n",
            "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.23.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.49->langchain-openai) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\n",
            "Downloading langchain_openai-0.3.12-py3-none-any.whl (61 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.3/61.3 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken, langchain-openai\n",
            "Successfully installed langchain-openai-0.3.12 tiktoken-0.9.0\n",
            "Requirement already satisfied: duckduckgo-search in /usr/local/lib/python3.11/dist-packages (2025.4.1)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (8.1.8)\n",
            "Requirement already satisfied: primp>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (0.14.0)\n",
            "Requirement already satisfied: lxml>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from duckduckgo-search) (5.3.1)\n",
            "Requirement already satisfied: openai in /usr/local/lib/python3.11/dist-packages (1.69.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai) (0.9.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.11/dist-packages (from openai) (2.11.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.11/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.11/dist-packages (from openai) (4.13.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install langchain\n",
        "! pip install langchain-community\n",
        "! pip install langchain-openai\n",
        "! pip install duckduckgo-search\n",
        "! pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6VXgYEWsiMJW",
        "outputId": "7248e175-104b-42fb-d38e-ff8db05345d9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================ CONFIGURACIÓN ================================\n",
            "❓ Pregunta: ¿Como programar y entrenar un mamba llm?\n",
            "🔢 Número de resultados: 4\n",
            "\n",
            "============================== INICIO DE PROCESO ==============================\n",
            "🔍 Procesando pregunta: ¿Como programar y entrenar un mamba llm?\n",
            "🔍 Número de resultados a procesar: 4\n",
            "\n",
            "=================================== BÚSQUEDA ===================================\n",
            "🔍 Realizando búsqueda en DuckDuckGo: '¿Como programar y entrenar un mamba llm?'\n",
            "🔍 Solicitando 4 resultados\n",
            "✅ Búsqueda completada en 0.82 segundos\n",
            "✅ Se encontraron 4 resultados:\n",
            "  [1] Introducción a la arquitectura Mamba LLM: Un nuevo ... - DataCamp\n",
            "      URL: https://www.datacamp.com/es/tutorial/introduction-to-the-mamba-llm-architecture\n",
            "  [2] Entrenamiento de LLM con Datasets Guía Paso a Paso\n",
            "      URL: https://decolornaranja.wordpress.com/2024/09/05/como-entrenar-un-modelo-de-lenguaje-llm-usando-un-dataset-paso-a-paso/\n",
            "  [3] Una guía paso a paso para entrenar modelos de lenguaje grandes (LLM ...\n",
            "      URL: https://blogs.novita.ai/es/a-step-by-step-guide-to-training-large-language-models-llms-on-your-own-data/\n",
            "  [4] Ejecutar modelos LLM en local sin GPU - Agentes de IA\n",
            "      URL: https://www.agentesdeia.com/ejecutar-modelos-llm-local/\n",
            "\n",
            "============================ ANÁLISIS DE RESULTADOS ============================\n",
            "🔍 Procesando 4 resultados de búsqueda\n",
            "\n",
            "============================= PROCESANDO PÁGINA 1 =============================\n",
            "📄 Cargando página 1: Introducción a la arquitectura Mamba LLM: Un nuevo ... - DataCamp\n",
            "📄 URL: https://www.datacamp.com/es/tutorial/introduction-to-the-mamba-llm-architecture\n",
            "📥 Descargando contenido...\n",
            "📥 Descarga completada en 0.12 segundos\n",
            "📊 ESTADÍSTICAS DEL DOCUMENTO:\n",
            "  • Caracteres: 57\n",
            "  • Palabras aproximadas: 8\n",
            "  • Tokens estimados: ~14 (estimación basada en 4 caracteres/token)\n",
            "✂️ Dividiendo documento en chunks...\n",
            "✂️ División completada en 0.00 segundos\n",
            "✂️ Documento dividido en 1 chunks\n",
            "  • Chunk 1: 57 caracteres, ~8 palabras, ~14 tokens\n",
            "ℹ️ La página 1 tiene un solo chunk, no requiere resumen\n",
            "\n",
            "============================= PROCESANDO PÁGINA 2 =============================\n",
            "📄 Cargando página 2: Entrenamiento de LLM con Datasets Guía Paso a Paso\n",
            "📄 URL: https://decolornaranja.wordpress.com/2024/09/05/como-entrenar-un-modelo-de-lenguaje-llm-usando-un-dataset-paso-a-paso/\n",
            "📥 Descargando contenido...\n",
            "📥 Descarga completada en 0.28 segundos\n",
            "📊 ESTADÍSTICAS DEL DOCUMENTO:\n",
            "  • Caracteres: 14,924\n",
            "  • Palabras aproximadas: 2,214\n",
            "  • Tokens estimados: ~3,731 (estimación basada en 4 caracteres/token)\n",
            "✂️ Dividiendo documento en chunks...\n",
            "✂️ División completada en 0.00 segundos\n",
            "✂️ Documento dividido en 6 chunks\n",
            "  • Chunk 1: 1,038 caracteres, ~153 palabras, ~259 tokens\n",
            "  • Chunk 2: 3,718 caracteres, ~582 palabras, ~929 tokens\n",
            "  • Chunk 3: 3,183 caracteres, ~484 palabras, ~795 tokens\n",
            "  • Chunk 4: 3,887 caracteres, ~614 palabras, ~971 tokens\n",
            "  • Chunk 5: 725 caracteres, ~107 palabras, ~181 tokens\n",
            "  • Chunk 6: 2,413 caracteres, ~284 palabras, ~603 tokens\n",
            "\n",
            "============================= RESUMIENDO PÁGINA 2 =============================\n",
            "📝 Resumiendo página 2: Entrenamiento de LLM con Datasets Guía Paso a Paso\n",
            "📝 Procesando 6 chunks\n",
            "🤖 Inicializando modelo GPT-3.5-turbo para resumir\n",
            "📋 Configurando prompts de mapeo y reducción\n",
            "⚙️ Creando cadena de resumen Map-Reduce\n",
            "🔄 Ejecutando cadena de resumen Map-Reduce...\n",
            "🔄 Fase de Map: Procesando 6 chunks individualmente\n",
            "✅ Resumen completado en 18.45 segundos\n",
            "📊 ESTADÍSTICAS DEL RESUMEN:\n",
            "  • Caracteres: 1,349\n",
            "  • Palabras aproximadas: 206\n",
            "  • Tokens estimados: ~337\n",
            "  • Ratio de compresión: 9.0% del original\n",
            "\n",
            "============================= PROCESANDO PÁGINA 3 =============================\n",
            "📄 Cargando página 3: Una guía paso a paso para entrenar modelos de lenguaje grandes (LLM ...\n",
            "📄 URL: https://blogs.novita.ai/es/a-step-by-step-guide-to-training-large-language-models-llms-on-your-own-data/\n",
            "📥 Descargando contenido...\n",
            "📥 Descarga completada en 1.31 segundos\n",
            "📊 ESTADÍSTICAS DEL DOCUMENTO:\n",
            "  • Caracteres: 22,303\n",
            "  • Palabras aproximadas: 3,309\n",
            "  • Tokens estimados: ~5,575 (estimación basada en 4 caracteres/token)\n",
            "✂️ Dividiendo documento en chunks...\n",
            "✂️ División completada en 0.00 segundos\n",
            "✂️ Documento dividido en 8 chunks\n",
            "  • Chunk 1: 3,789 caracteres, ~530 palabras, ~947 tokens\n",
            "  • Chunk 2: 2,273 caracteres, ~367 palabras, ~568 tokens\n",
            "  • Chunk 3: 3,817 caracteres, ~580 palabras, ~954 tokens\n",
            "  • Chunk 4: 249 caracteres, ~35 palabras, ~62 tokens\n",
            "  • Chunk 5: 3,818 caracteres, ~589 palabras, ~954 tokens\n",
            "  • Chunk 6: 3,400 caracteres, ~494 palabras, ~850 tokens\n",
            "  • Chunk 7: 3,857 caracteres, ~564 palabras, ~964 tokens\n",
            "  • Chunk 8: 1,242 caracteres, ~173 palabras, ~310 tokens\n",
            "\n",
            "============================= RESUMIENDO PÁGINA 3 =============================\n",
            "📝 Resumiendo página 3: Una guía paso a paso para entrenar modelos de lenguaje grandes (LLM ...\n",
            "📝 Procesando 8 chunks\n",
            "🤖 Inicializando modelo GPT-3.5-turbo para resumir\n",
            "📋 Configurando prompts de mapeo y reducción\n",
            "⚙️ Creando cadena de resumen Map-Reduce\n",
            "🔄 Ejecutando cadena de resumen Map-Reduce...\n",
            "🔄 Fase de Map: Procesando 8 chunks individualmente\n",
            "✅ Resumen completado en 32.28 segundos\n",
            "📊 ESTADÍSTICAS DEL RESUMEN:\n",
            "  • Caracteres: 2,292\n",
            "  • Palabras aproximadas: 330\n",
            "  • Tokens estimados: ~573\n",
            "  • Ratio de compresión: 10.3% del original\n",
            "\n",
            "============================= PROCESANDO PÁGINA 4 =============================\n",
            "📄 Cargando página 4: Ejecutar modelos LLM en local sin GPU - Agentes de IA\n",
            "📄 URL: https://www.agentesdeia.com/ejecutar-modelos-llm-local/\n",
            "📥 Descargando contenido...\n",
            "📥 Descarga completada en 1.01 segundos\n",
            "📊 ESTADÍSTICAS DEL DOCUMENTO:\n",
            "  • Caracteres: 10,484\n",
            "  • Palabras aproximadas: 1,641\n",
            "  • Tokens estimados: ~2,621 (estimación basada en 4 caracteres/token)\n",
            "✂️ Dividiendo documento en chunks...\n",
            "✂️ División completada en 0.00 segundos\n",
            "✂️ Documento dividido en 4 chunks\n",
            "  • Chunk 1: 2,242 caracteres, ~351 palabras, ~560 tokens\n",
            "  • Chunk 2: 2,444 caracteres, ~401 palabras, ~611 tokens\n",
            "  • Chunk 3: 3,673 caracteres, ~594 palabras, ~918 tokens\n",
            "  • Chunk 4: 2,382 caracteres, ~350 palabras, ~595 tokens\n",
            "\n",
            "============================= RESUMIENDO PÁGINA 4 =============================\n",
            "📝 Resumiendo página 4: Ejecutar modelos LLM en local sin GPU - Agentes de IA\n",
            "📝 Procesando 4 chunks\n",
            "🤖 Inicializando modelo GPT-3.5-turbo para resumir\n",
            "📋 Configurando prompts de mapeo y reducción\n",
            "⚙️ Creando cadena de resumen Map-Reduce\n",
            "🔄 Ejecutando cadena de resumen Map-Reduce...\n",
            "🔄 Fase de Map: Procesando 4 chunks individualmente\n",
            "✅ Resumen completado en 13.56 segundos\n",
            "📊 ESTADÍSTICAS DEL RESUMEN:\n",
            "  • Caracteres: 1,075\n",
            "  • Palabras aproximadas: 162\n",
            "  • Tokens estimados: ~268\n",
            "  • Ratio de compresión: 10.3% del original\n",
            "\n",
            "======================== PREPARACIÓN DEL CONTEXTO FINAL ========================\n",
            "📋 Preparando contexto final para respuesta...\n",
            "  • Fuente 1: Introducción a la arquitectura Mamba LLM: Un nuevo ... - DataCamp (57 caracteres)\n",
            "  • Fuente 2: Entrenamiento de LLM con Datasets Guía Paso a Paso (1,349 caracteres)\n",
            "  • Fuente 3: Una guía paso a paso para entrenar modelos de lenguaje grandes (LLM ... (2,292 caracteres)\n",
            "  • Fuente 4: Ejecutar modelos LLM en local sin GPU - Agentes de IA (1,075 caracteres)\n",
            "📊 ESTADÍSTICAS DEL CONTEXTO FINAL:\n",
            "  • Total caracteres: 5,469\n",
            "  • Tokens estimados: ~1,367\n",
            "\n",
            "======================== GENERACIÓN DE RESPUESTA FINAL ========================\n",
            "🤖 Inicializando modelo para respuesta final...\n",
            "🤖 Usando modelo gpt-3.5-turbo estándar\n",
            "🔄 Enviando contexto a GPT para respuesta final...\n",
            "✅ Respuesta generada en 10.56 segundos\n",
            "📊 ESTADÍSTICAS DE LA RESPUESTA FINAL:\n",
            "  • Caracteres: 2,526\n",
            "  • Palabras aproximadas: 372\n",
            "\n",
            "============================== PROCESO COMPLETADO ==============================\n",
            "⏱️ Tiempo total de ejecución: 78.90 segundos\n",
            "\n",
            "=============================== RESULTADO FINAL ===============================\n",
            "Para programar y entrenar un modelo de lenguaje grande (LLM) como un Mamba LLM, es fundamental seguir una serie de pasos y consideraciones clave. \n",
            "\n",
            "1. **Selección del dataset adecuado:** Se debe elegir un dataset diverso y de alta calidad, posiblemente etiquetado dependiendo de la tarea específica [FUENTE 2].\n",
            "\n",
            "2. **Preprocesamiento de datos:** Es importante realizar limpieza, tokenización, normalización y filtrado de los datos antes de entrenar el modelo [FUENTE 2].\n",
            "\n",
            "3. **Configuración del modelo:** Se debe elegir una arquitectura adecuada como GPT, BERT o T5 y ajustar los hiperparámetros durante el entrenamiento utilizando algoritmos de optimización como Adam o SGD [FUENTE 2].\n",
            "\n",
            "4. **Entrenamiento y evaluación:** Durante el entrenamiento, se debe monitorear el modelo para evitar el sobreajuste y evaluarlo en un conjunto de datos de prueba. Se pueden ajustar hiperparámetros o la arquitectura si es necesario [FUENTE 2].\n",
            "\n",
            "5. **Selección de marco de aprendizaje profundo y arquitectura:** Es crucial elegir un marco como TensorFlow, PyTorch o Hugging Face Transformers, y utilizar la arquitectura Transformer como punto de partida común para un Mamba LLM [FUENTE 3].\n",
            "\n",
            "6. **Recursos y tiempo de entrenamiento:** Se debe tener en cuenta que el entrenamiento de un modelo LLM puede consumir recursos computacionales y tiempo considerable, con duraciones que van desde horas hasta semanas [FUENTE 3].\n",
            "\n",
            "7. **Validación y ajuste fino:** Es importante validar y evaluar periódicamente el rendimiento del modelo, ajustarlo finamente si es necesario, y probarlo con datos del mundo real antes de implementarlo para los usuarios [FUENTE 3].\n",
            "\n",
            "8. **Herramientas y modelos disponibles:** Se pueden utilizar herramientas como GPT4All, LMStudio, AnythingLLM y Ollama para programar y entrenar un modelo LLM, con requisitos de hardware específicos. Además, existen modelos como Gemma, Llama, Phi3, Openchat, entre otros, que se pueden utilizar en experimentos [FUENTE 4].\n",
            "\n",
            "En resumen, para programar y entrenar un Mamba LLM, se deben seguir los pasos mencionados anteriormente, considerando la selección del dataset, el preprocesamiento de los datos, la configuración del modelo, el entrenamiento, la evaluación, la selección de marco de aprendizaje y arquitectura, la gestión de recursos y tiempo, la validación y ajuste fino, y la utilización de herramientas y modelos disponibles. Se recomienda consultar fuentes adicionales para obtener información más detallada sobre la programación y entrenamiento específico de un Mamba LLM.\n",
            "\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "from langchain_community.utilities import DuckDuckGoSearchAPIWrapper\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain.chains.summarize import load_summarize_chain\n",
        "from langchain.chains import LLMChain\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.documents import Document\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "# Configurar la API key de OpenAI\n",
        "os.environ[\"OPENAI_API_KEY\"]\n",
        "\n",
        "def log_separator(message=\"\"):\n",
        "    \"\"\"Imprime un separador con un mensaje opcional\"\"\"\n",
        "    width = 80\n",
        "    if message:\n",
        "        padding = (width - len(message) - 2) // 2\n",
        "        print(\"\\n\" + \"=\" * padding + f\" {message} \" + \"=\" * padding)\n",
        "    else:\n",
        "        print(\"\\n\" + \"=\" * width)\n",
        "\n",
        "def realizar_busqueda(query: str, num_results: int = 3) -> List[Dict[str, Any]]:\n",
        "    \"\"\"\n",
        "    Realiza una búsqueda en DuckDuckGo y devuelve los resultados\n",
        "    \"\"\"\n",
        "    log_separator(\"BÚSQUEDA\")\n",
        "    print(f\"🔍 Realizando búsqueda en DuckDuckGo: '{query}'\")\n",
        "    print(f\"🔍 Solicitando {num_results} resultados\")\n",
        "\n",
        "    start_time = time.time()\n",
        "    search = DuckDuckGoSearchAPIWrapper()\n",
        "    results = search.results(query, num_results)\n",
        "    elapsed = time.time() - start_time\n",
        "\n",
        "    print(f\"✅ Búsqueda completada en {elapsed:.2f} segundos\")\n",
        "    print(f\"✅ Se encontraron {len(results)} resultados:\")\n",
        "\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"  [{i}] {result.get('title', 'Sin título')}\")\n",
        "        print(f\"      URL: {result.get('link', 'Sin enlace')}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def cargar_y_procesar_pagina(url: str, title: str, index: int) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Carga y procesa una página web completa, dividiéndola en chunks si es necesario\n",
        "    \"\"\"\n",
        "    log_separator(f\"PROCESANDO PÁGINA {index}\")\n",
        "    print(f\"📄 Cargando página {index}: {title}\")\n",
        "    print(f\"📄 URL: {url}\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        print(f\"📥 Descargando contenido...\")\n",
        "        loader = WebBaseLoader(url)\n",
        "        documents = loader.load()\n",
        "        download_time = time.time() - start_time\n",
        "        print(f\"📥 Descarga completada en {download_time:.2f} segundos\")\n",
        "\n",
        "        if not documents:\n",
        "            print(f\"⚠️ AVISO: No se pudo extraer contenido de {url}\")\n",
        "            return {\n",
        "                \"title\": title,\n",
        "                \"url\": url,\n",
        "                \"success\": False,\n",
        "                \"content\": None,\n",
        "                \"chunks\": [],\n",
        "                \"error\": \"No se pudo extraer contenido\",\n",
        "                \"stats\": {\n",
        "                    \"download_time\": download_time,\n",
        "                    \"processing_time\": 0,\n",
        "                    \"total_time\": download_time\n",
        "                }\n",
        "            }\n",
        "\n",
        "        content = documents[0].page_content\n",
        "        content_chars = len(content)\n",
        "        content_words = len(content.split())\n",
        "\n",
        "        print(f\"📊 ESTADÍSTICAS DEL DOCUMENTO:\")\n",
        "        print(f\"  • Caracteres: {content_chars:,}\")\n",
        "        print(f\"  • Palabras aproximadas: {content_words:,}\")\n",
        "        print(f\"  • Tokens estimados: ~{int(content_chars/4):,} (estimación basada en 4 caracteres/token)\")\n",
        "\n",
        "        # Dividir el contenido en chunks manejables\n",
        "        print(f\"✂️ Dividiendo documento en chunks...\")\n",
        "        chunk_start = time.time()\n",
        "\n",
        "        text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=4000,         # Tamaño aproximado de cada chunk\n",
        "            chunk_overlap=200,       # Solapamiento entre chunks para mantener contexto\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],  # Prioridad de separadores\n",
        "        )\n",
        "\n",
        "        chunks = text_splitter.split_text(content)\n",
        "        chunk_time = time.time() - chunk_start\n",
        "\n",
        "        print(f\"✂️ División completada en {chunk_time:.2f} segundos\")\n",
        "        print(f\"✂️ Documento dividido en {len(chunks)} chunks\")\n",
        "\n",
        "        # Mostrar estadísticas de los chunks\n",
        "        for i, chunk in enumerate(chunks, 1):\n",
        "            chunk_chars = len(chunk)\n",
        "            chunk_words = len(chunk.split())\n",
        "            print(f\"  • Chunk {i}: {chunk_chars:,} caracteres, ~{chunk_words:,} palabras, ~{int(chunk_chars/4):,} tokens\")\n",
        "\n",
        "        total_time = time.time() - start_time\n",
        "\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"url\": url,\n",
        "            \"success\": True,\n",
        "            \"content\": content,  # Contenido completo\n",
        "            \"chunks\": chunks,    # Contenido dividido en chunks\n",
        "            \"error\": None,\n",
        "            \"stats\": {\n",
        "                \"download_time\": download_time,\n",
        "                \"processing_time\": chunk_time,\n",
        "                \"total_time\": total_time,\n",
        "                \"total_chars\": content_chars,\n",
        "                \"total_words\": content_words,\n",
        "                \"num_chunks\": len(chunks),\n",
        "                \"chunk_sizes\": [len(chunk) for chunk in chunks]\n",
        "            }\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"❌ ERROR: Error al procesar {url}: {str(e)}\")\n",
        "        return {\n",
        "            \"title\": title,\n",
        "            \"url\": url,\n",
        "            \"success\": False,\n",
        "            \"content\": None,\n",
        "            \"chunks\": [],\n",
        "            \"error\": str(e),\n",
        "            \"stats\": {\n",
        "                \"download_time\": elapsed,\n",
        "                \"processing_time\": 0,\n",
        "                \"total_time\": elapsed\n",
        "            }\n",
        "        }\n",
        "\n",
        "def resumir_pagina_larga(pagina: Dict[str, Any], pregunta: str, index: int) -> str:\n",
        "    \"\"\"\n",
        "    Procesa una página muy larga dividida en chunks y produce un resumen\n",
        "    \"\"\"\n",
        "    if not pagina[\"success\"] or not pagina[\"chunks\"]:\n",
        "        return f\"No se pudo analizar la página: {pagina['error'] or 'Contenido no disponible'}\"\n",
        "\n",
        "    log_separator(f\"RESUMIENDO PÁGINA {index}\")\n",
        "    print(f\"📝 Resumiendo página {index}: {pagina['title']}\")\n",
        "    print(f\"📝 Procesando {len(pagina['chunks'])} chunks\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Convertir los chunks a formato Document de LangChain\n",
        "    chunks_docs = [\n",
        "        Document(page_content=chunk, metadata={\"source\": pagina[\"url\"], \"title\": pagina[\"title\"]})\n",
        "        for chunk in pagina[\"chunks\"]\n",
        "    ]\n",
        "\n",
        "    # Inicializar el modelo\n",
        "    print(f\"🤖 Inicializando modelo GPT-3.5-turbo para resumir\")\n",
        "    llm = ChatOpenAI(temperature=0.3, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "    # Prompt para resumir cada chunk\n",
        "    print(f\"📋 Configurando prompts de mapeo y reducción\")\n",
        "    map_prompt_template = \"\"\"\n",
        "    Tu tarea es extraer información relevante del siguiente fragmento de una página web\n",
        "    que responda a esta pregunta del usuario: \"{question}\"\n",
        "\n",
        "    Solo extrae información directamente relacionada con la pregunta. Sé conciso.\n",
        "\n",
        "    FRAGMENTO DE PÁGINA WEB:\n",
        "    {text}\n",
        "\n",
        "    INFORMACIÓN RELEVANTE EXTRAÍDA (solo si está relacionada con la pregunta):\n",
        "    \"\"\"\n",
        "\n",
        "    map_prompt = PromptTemplate(\n",
        "        template=map_prompt_template,\n",
        "        input_variables=[\"text\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Prompt para combinar los resúmenes de los chunks\n",
        "    combine_prompt_template = \"\"\"\n",
        "    Has recibido extractos de información de diferentes partes de una página web titulada \"{title}\".\n",
        "    Usa estos extractos para crear un resumen coherente que responda a la pregunta del usuario.\n",
        "\n",
        "    PREGUNTA DEL USUARIO: {question}\n",
        "\n",
        "    EXTRACTOS DE LA PÁGINA WEB:\n",
        "    {text}\n",
        "\n",
        "    RESUMEN COHERENTE Y COMPLETO:\n",
        "    \"\"\"\n",
        "\n",
        "    combine_prompt = PromptTemplate(\n",
        "        template=combine_prompt_template,\n",
        "        input_variables=[\"title\", \"text\", \"question\"]\n",
        "    )\n",
        "\n",
        "    # Cargar la cadena de resumen\n",
        "    print(f\"⚙️ Creando cadena de resumen Map-Reduce\")\n",
        "    summary_chain = load_summarize_chain(\n",
        "        llm,\n",
        "        chain_type=\"map_reduce\",\n",
        "        map_prompt=map_prompt,\n",
        "        combine_prompt=combine_prompt,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        # Ejecutar la cadena de resumen con los chunks\n",
        "        print(f\"🔄 Ejecutando cadena de resumen Map-Reduce...\")\n",
        "        print(f\"🔄 Fase de Map: Procesando {len(chunks_docs)} chunks individualmente\")\n",
        "        summary_start = time.time()\n",
        "\n",
        "        summary = summary_chain.invoke({\n",
        "            \"input_documents\": chunks_docs,\n",
        "            \"question\": pregunta,\n",
        "            \"title\": pagina[\"title\"]\n",
        "        })\n",
        "\n",
        "        summary_time = time.time() - summary_start\n",
        "        print(f\"✅ Resumen completado en {summary_time:.2f} segundos\")\n",
        "\n",
        "        # Extraer el resultado\n",
        "        if isinstance(summary, dict) and \"output_text\" in summary:\n",
        "            result = summary[\"output_text\"]\n",
        "        else:\n",
        "            result = str(summary)\n",
        "\n",
        "        result_chars = len(result)\n",
        "        result_words = len(result.split())\n",
        "\n",
        "        print(f\"📊 ESTADÍSTICAS DEL RESUMEN:\")\n",
        "        print(f\"  • Caracteres: {result_chars:,}\")\n",
        "        print(f\"  • Palabras aproximadas: {result_words:,}\")\n",
        "        print(f\"  • Tokens estimados: ~{int(result_chars/4):,}\")\n",
        "        print(f\"  • Ratio de compresión: {result_chars/pagina['stats']['total_chars']*100:.1f}% del original\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"❌ ERROR: Error al resumir: {str(e)}\")\n",
        "        return f\"Error al resumir la página: {str(e)}\"\n",
        "\n",
        "def analizar_resultados_completos(resultados_busqueda: List[Dict[str, Any]], pregunta: str) -> str:\n",
        "    \"\"\"\n",
        "    Procesa los resultados de búsqueda, carga cada página web, y genera una respuesta\n",
        "    \"\"\"\n",
        "    log_separator(\"ANÁLISIS DE RESULTADOS\")\n",
        "    print(f\"🔍 Procesando {len(resultados_busqueda)} resultados de búsqueda\")\n",
        "\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Procesar cada página encontrada\n",
        "    paginas_procesadas = []\n",
        "\n",
        "    for i, result in enumerate(resultados_busqueda, 1):\n",
        "        title = result.get(\"title\", \"Sin título\")\n",
        "        url = result.get(\"link\", \"Sin enlace\")\n",
        "\n",
        "        # Cargar y procesar la página\n",
        "        pagina_procesada = cargar_y_procesar_pagina(url, title, i)\n",
        "\n",
        "        if pagina_procesada[\"success\"]:\n",
        "            # Si la página es muy larga (tiene múltiples chunks), resumirla\n",
        "            if len(pagina_procesada[\"chunks\"]) > 1:\n",
        "                resumen = resumir_pagina_larga(pagina_procesada, pregunta, i)\n",
        "                pagina_procesada[\"resumen\"] = resumen\n",
        "            # Si la página es corta, usar el contenido directamente\n",
        "            else:\n",
        "                print(f\"ℹ️ La página {i} tiene un solo chunk, no requiere resumen\")\n",
        "                pagina_procesada[\"resumen\"] = pagina_procesada[\"content\"]\n",
        "\n",
        "        paginas_procesadas.append(pagina_procesada)\n",
        "\n",
        "    # Preparar el contexto final para GPT\n",
        "    log_separator(\"PREPARACIÓN DEL CONTEXTO FINAL\")\n",
        "    print(f\"📋 Preparando contexto final para respuesta...\")\n",
        "\n",
        "    contexto_items = []\n",
        "    total_contexto_chars = 0\n",
        "\n",
        "    for i, pagina in enumerate(paginas_procesadas, 1):\n",
        "        if pagina[\"success\"]:\n",
        "            contenido = pagina.get('resumen', pagina['content'])\n",
        "            contexto_item = f\"[FUENTE {i}] {pagina['title']}\\n[URL] {pagina['url']}\\n\\n{contenido}\"\n",
        "            chars = len(contenido)\n",
        "            total_contexto_chars += chars\n",
        "            print(f\"  • Fuente {i}: {pagina['title']} ({chars:,} caracteres)\")\n",
        "        else:\n",
        "            contexto_item = f\"[FUENTE {i}] {pagina['title']}\\n[URL] {pagina['url']}\\n\\nNo se pudo obtener el contenido: {pagina['error']}\"\n",
        "            print(f\"  • Fuente {i}: {pagina['title']} (NO DISPONIBLE - {pagina['error']})\")\n",
        "\n",
        "        contexto_items.append(contexto_item)\n",
        "\n",
        "    # Unir el contexto final\n",
        "    contexto_final = \"\\n\\n---\\n\\n\".join(contexto_items)\n",
        "    contexto_final_chars = len(contexto_final)\n",
        "    contexto_final_tokens = int(contexto_final_chars/4)\n",
        "\n",
        "    print(f\"📊 ESTADÍSTICAS DEL CONTEXTO FINAL:\")\n",
        "    print(f\"  • Total caracteres: {contexto_final_chars:,}\")\n",
        "    print(f\"  • Tokens estimados: ~{contexto_final_tokens:,}\")\n",
        "\n",
        "    if contexto_final_tokens > 15000:\n",
        "        print(f\"⚠️ ADVERTENCIA: El contexto final excede los 15,000 tokens aproximados.\")\n",
        "        print(f\"⚠️ Se usará gpt-3.5-turbo-16k que puede manejar hasta ~16k tokens.\")\n",
        "\n",
        "    # Generar la respuesta final con GPT\n",
        "    log_separator(\"GENERACIÓN DE RESPUESTA FINAL\")\n",
        "    print(f\"🤖 Inicializando modelo para respuesta final...\")\n",
        "\n",
        "    template = \"\"\"\n",
        "    Responde a la siguiente pregunta basándote en el contenido de las páginas web proporcionadas.\n",
        "    Para cada afirmación importante en tu respuesta, cita la fuente correspondiente usando [FUENTE X].\n",
        "\n",
        "    PREGUNTA: {question}\n",
        "\n",
        "    CONTENIDO DE LAS PÁGINAS WEB:\n",
        "    {context}\n",
        "\n",
        "    RESPUESTA DETALLADA (citando las fuentes):\n",
        "    \"\"\"\n",
        "\n",
        "    prompt = PromptTemplate(\n",
        "        input_variables=[\"context\", \"question\"],\n",
        "        template=template\n",
        "    )\n",
        "\n",
        "    # Usar un modelo con contexto amplio para la respuesta final\n",
        "    if contexto_final_tokens > 15000:\n",
        "        print(f\"🤖 Usando modelo gpt-3.5-turbo-16k para manejar el contexto amplio\")\n",
        "        llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo-16k\")\n",
        "    else:\n",
        "        print(f\"🤖 Usando modelo gpt-3.5-turbo estándar\")\n",
        "        llm = ChatOpenAI(temperature=0.7, model_name=\"gpt-3.5-turbo\")\n",
        "\n",
        "    chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "    try:\n",
        "        print(f\"🔄 Enviando contexto a GPT para respuesta final...\")\n",
        "        resp_start = time.time()\n",
        "\n",
        "        respuesta = chain.invoke({\n",
        "            \"context\": contexto_final,\n",
        "            \"question\": pregunta\n",
        "        })\n",
        "\n",
        "        resp_time = time.time() - resp_start\n",
        "        print(f\"✅ Respuesta generada en {resp_time:.2f} segundos\")\n",
        "\n",
        "        if isinstance(respuesta, dict) and \"text\" in respuesta:\n",
        "            result = respuesta[\"text\"]\n",
        "        else:\n",
        "            result = respuesta\n",
        "\n",
        "        result_chars = len(result)\n",
        "        result_words = len(result.split())\n",
        "\n",
        "        print(f\"📊 ESTADÍSTICAS DE LA RESPUESTA FINAL:\")\n",
        "        print(f\"  • Caracteres: {result_chars:,}\")\n",
        "        print(f\"  • Palabras aproximadas: {result_words:,}\")\n",
        "\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ ERROR: Error al generar respuesta final: {str(e)}\")\n",
        "        return f\"Error al generar la respuesta: {str(e)}\"\n",
        "\n",
        "def buscar_y_responder(pregunta: str, num_resultados: int = 4) -> str:\n",
        "    \"\"\"\n",
        "    Función principal que gestiona todo el proceso\n",
        "    \"\"\"\n",
        "    log_separator(\"INICIO DE PROCESO\")\n",
        "    print(f\"🔍 Procesando pregunta: {pregunta}\")\n",
        "    print(f\"🔍 Número de resultados a procesar: {num_resultados}\")\n",
        "\n",
        "    global_start = time.time()\n",
        "\n",
        "    # Realizar búsqueda\n",
        "    resultados = realizar_busqueda(pregunta, num_resultados)\n",
        "\n",
        "    # Analizar resultados y generar respuesta\n",
        "    respuesta = analizar_resultados_completos(resultados, pregunta)\n",
        "\n",
        "    total_time = time.time() - global_start\n",
        "\n",
        "    log_separator(\"PROCESO COMPLETADO\")\n",
        "    print(f\"⏱️ Tiempo total de ejecución: {total_time:.2f} segundos\")\n",
        "\n",
        "    return respuesta\n",
        "\n",
        "# Ejemplo de uso\n",
        "if __name__ == \"__main__\":\n",
        "    pregunta_usuario = \"¿Como programar y entrenar un mamba llm?\"\n",
        "    num_resultados = 4\n",
        "\n",
        "    log_separator(\"CONFIGURACIÓN\")\n",
        "    print(f\"❓ Pregunta: {pregunta_usuario}\")\n",
        "    print(f\"🔢 Número de resultados: {num_resultados}\")\n",
        "\n",
        "    respuesta = buscar_y_responder(pregunta_usuario, num_resultados)\n",
        "\n",
        "    log_separator(\"RESULTADO FINAL\")\n",
        "    print(respuesta)\n",
        "    log_separator()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
